# Azure AD App Registration Configuration
# Get these values from https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps

VITE_AZURE_CLIENT_ID="your-client-id"
VITE_AZURE_TENANT_ID="your-tenant-id"

# Azure OpenAI Configuration (Optional - enables AI-powered summaries)
# Get these values from your Azure OpenAI resource in Azure Portal
# https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.CognitiveServices%2Faccounts

# Your Azure OpenAI endpoint (e.g., https://your-resource-name.openai.azure.com)
VITE_AZURE_OPENAI_ENDPOINT=""

# Your Azure OpenAI API key
VITE_AZURE_OPENAI_API_KEY=""

# Your deployment name (the name you gave when deploying the model, e.g., gpt-35-turbo, gpt-4)
VITE_AZURE_OPENAI_DEPLOYMENT="gpt-35-turbo"

# API version (default: 2024-02-15-preview)
VITE_AZURE_OPENAI_API_VERSION="2024-02-15-preview"

# ============================================
# Groq Configuration (RECOMMENDED - Fast, Free, Llama 3.1 70B)
# ============================================
# Get your API key from: https://console.groq.com/keys
# Free tier: 14,400 requests/day, very fast inference

VITE_GROQ_API_KEY=""

# Model options: llama-3.1-70b-versatile (best), llama-3.1-8b-instant (faster), mixtral-8x7b-32768
VITE_GROQ_MODEL="llama-3.1-70b-versatile"

# ============================================
# Google Gemini Configuration (Backup option)
# ============================================
# Get your API key from: https://aistudio.google.com/app/apikey
# Free tier: 60 requests/minute, 1500 requests/day

VITE_GEMINI_API_KEY=""

# Model options: gemini-2.0-flash (fast, free), gemini-1.5-pro-latest (more capable)
VITE_GEMINI_MODEL="gemini-2.0-flash"

# ============================================
# Ollama Configuration (Local, free, private)
# ============================================
# Ollama runs locally - install from https://ollama.ai
# Then run: ollama pull llama3

# Ollama server URL (default: http://localhost:11434)
VITE_OLLAMA_BASE_URL="http://localhost:11434"

# Model to use (e.g., llama3, mistral, phi, gemma, llama2)
VITE_OLLAMA_MODEL="llama3"

# Request timeout in milliseconds (default: 60000)
VITE_OLLAMA_TIMEOUT="60000"
